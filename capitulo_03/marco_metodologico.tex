\fancyhead{}
\fancyfoot{}
\cfoot{\thepage}


\lhead{Método}
% \chapter{Método}
\chapter{Predicción de la  demanda}
En esta sección, se describirán los métodos utilizados para llevar a cabo la investigación sobre la predicción de compras a parir de un historial de ventas.

\section{Preprocesamiento de datos}
Los datos usados para generar el modelo fueron proporcionados por la empresa gastronomica. La misma proporcionó el dataset para ser usado con fines académicos, los datos presentan las ventas diarias para los diferentes productos  cuenta con 6 meses de registro desde el 17 de abril del 2023 hasta el 15 de octubre del 2023.

Los datos originales suministrados están conformados por los siguientes datasets: 
\begin{itemize}
  \item productos.csv 
  \item ventas.csv
\end{itemize}

Los datos vienen organizados en forma tabular, cada archivo presenta información separada relacionada con los diferentes productos y el registro de ventas histórico.

\vspace{1\baselineskip}
\textbf{productos.csv}

Estructura inicial de la tabla de productos (informacion de los 5 primeros productos(Fig. \ref{fig:priemeros_5productos})).

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./tabla_producto.png}
    \caption{Estructura inicial de la tabla productos.csv}
    \label{fig:priemeros_5productos}
  \end{center}
\end{figure}

\textbf{ventas.csv}

Estructura inicial de la tabla de ventas(con información de 6 registros (Fig. \ref{fig:ventas_original})).

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./ventas_tabla.png}
    \caption{Estructura inicial de la tabla  ventas.csv}
    \label{fig:ventas_original}
  \end{center}
\end{figure}

%MODELADO
\subsection{Modelado de datos}
En el siguiente apartado se estara realizando el proceso de limpieza de los datos para la optención de los parametros a utilizar.

\vspace{1\baselineskip}

Se obtuvieron las columnas necesarias de la tabla productos para el desarrolo del proyecto, desde la base de datos MySQL herramienta utilizada para el almacenamiento de los datos, mediante la ejecución de los siguientes SQL query:

\vspace{4\baselineskip}
La siguiente consulta SQL selecciona las columnas \texttt{id} y \texttt{producto} de la tabla \texttt{productos}, donde la columna \texttt{anulado} tiene un valor nulo (NULL)(Figura:\ref{fig:priemeros_5productos}):
\begin{center}
  \begin{verbatim}
    SELECT id, producto
    FROM productos
    WHERE anulado IS NULL;
  \end{verbatim}
\end{center}
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./primeros_5productos.png}
    \caption{Datos de la tabla productos(Los primeros 5 productos)}
    \label{fig:priemeros_5productos}
  \end{center}
\end{figure}


En la tabla \ref{tab:productos} se muestra una descripción de cada una de las columnas de productos.

\begin{table}[H]

  \begin{tabular}{|c|l|}  % Usamos "l" para alinear a la izquierda
    \hline
    \rowcolor{gray!50} \textbf{Columna} & \textbf{Descripción} \\
    \hline
    id &  el id del producto (identificador unico)\\
    producto & el nombre del producto \\
    \hline
  \end{tabular}
  \centering
  \caption{ Descripción de datos de la tabla de productos}
  \label{tab:productos} % Asigna una etiqueta a la tabla
\end{table}

La consulta SQL a continuación selecciona las columnas \texttt{id\_producto}, \texttt{cantidad}, y \texttt{fecha\_venta} de la tabla \texttt{ventas} para las transacciones que tuvieron lugar en el período desde el 17 de abril de 2023 hasta el 15 de octubre de 2023 donde la columna \texttt{anulado} tiene un valor nulo (NULL)(Figura:\ref{fig:fecha_venta}):

\begin{verbatim}
  SELECT id_producto, cantidad, fecha_venta
  FROM ventas
  WHERE fecha_venta BETWEEN '2023-04-17' AND '2023-10-15' AND anulado IS NUL;
\end{verbatim}
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./ventas_fecha.png}
    \caption{Datos de la tabla ventas}
    \label{fig:fecha_venta}
  \end{center}
\end{figure}

En la tabla \ref{tab:ventas_fechas} se muestra una descripción de cada una de las columnas de productos.

\begin{table}[H]

  \begin{tabular}{|c|l|}  % Usamos "l" para alinear a la izquierda
    \hline
    \rowcolor{gray!50} \textbf{Columna} & \textbf{Descripción} \\
    \hline
    id\_producto &  el id del producto (identificador unico)\\
    catidad & cantidad de venta \\
    fecha\_venta & fecha de la venta \\
    \hline
  \end{tabular}
  \centering
  \caption{ Descripción de datos de la tabla ventas}
  \label{tab:ventas_fechas} % Asigna una etiqueta a la tabla
\end{table}


%ESCALADO

\subsection{Escalado de datos}

El escalado de datos es un proceso esencial en el análisis de datos y la estadística que ajusta los valores de las variables para que compartan una misma escala o propiedades específicas.

La consulta SQL se enfoca en el escalado de datos de ventas de productos durante un periodo del 17 de abril al 15 de octubre, excluyendo ventas anuladas. La consulta agrupa los resultados por producto y fecha de venta, permitiendo así obtener una visión escalada de la cantidad total vendida de cada producto a lo largo del tiempo, lo que facilita la identificación de tendencias y patrones en las ventas.

\begin{verbatim}
  SELECT id_producto, SUM(cantidad) AS Cantidad, fecha_venta
  FROM ventas
  WHERE fecha_venta BETWEEN '2023-04-17' AND '2023-10-15'
  AND anulado IS NULL
  GROUP BY id_producto, fecha_venta;
\end{verbatim}


\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./escalado.png}
    \caption{Datos escalados(muestra de la tabla ventas)}
    \label{fig:escalado}
  \end{center}
\end{figure}

\begin{table}[H]

  \begin{tabular}{|c|l|}  % Usamos "l" para alinear a la izquierda
    \hline
    \rowcolor{gray!50} \textbf{Columna} & \textbf{Descripción} \\
    \hline
    id\_producto &  el id del producto\\
    catidad & suma de la cantidad de ventas por fecha y producto \\
    fecha\_venta & fecha de la venta \\
    \hline
  \end{tabular}
  \centering
  \caption{ Descripción de datos escalados de la tabla ventas }
  \label{tab:tabla_scalada} % Asigna una etiqueta a la tabla
\end{table}


%ANALISIS DE LOS DATOS
\subsection{Análisis de los datos }
El objetivo principal de esta etapa es explorar y comprender en profundidad los datos, lo que resulta fundamental para una sólida preparación y análisis de los mismos. Esta comprensión más profunda del problema de negocio nos permitirá seleccionar modelos de predicción y tomar decisiones más fundamentadas. Durante el proceso de limpieza y preparación de los datos, se han identificado características relevantes que proporcionarán información valiosa para las fases posteriores del análisis y la toma de decisiones.

Como se observa(Figura: \ref{fig:fecha_venta}), la base de datos de entrenamiento es limitada en términos de variables disponibles. En este escenario, se ha seleccionado exclusivamente la fecha como variable de entrada y se ha focalizado en un producto específico, en este caso, la “Hamburguesa simple” con id\_producto 49, debido a su alta demanda. Se han recopilado 182 registros ya agrupados por día, lo que equivale a 182 días de datos. El propósito de este enfoque es analizar el comportamiento de las ventas de dicho producto en relación al tiempo y buscar patrones que puedan mejorar la precisión del modelo de predicción.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.40]{./serie_normal_completa.png}
    \caption{Serie temporal de las ventas totales por día}
    \label{fig:serie_completa}
  \end{center}
\end{figure}



%   En la tabla \ref{tab:ventas} se muestra una descripción de cada una de las columnas de ventas.
%   

% \section{Enfoque}
% El desarrollo de un sistema de compras inteligentes que utiliza un histórico de ventas como fuente principal de información. El objetivo principal es optimizar la gestión de insumos gastronómicos.

% \begin{itemize}
% \item Recopilación y análisis de datos cuantitativos de ventas históricas para desarrollar un modelo predictivo. Esto abarcaría la identificación de patrones de ventas, el uso de algoritmos de aprendizaje automático para predecir futuras compras y la evaluación de la precisión del sistema. 
% \item Aplicación de métricas cuantitativas para evaluar el rendimiento del sistema en términos de eficiencia, impacto en las ventas y beneficios para la entidad objeto de estudio.

% \end{itemize}

% \section{Alcance de la investigación cuantitativa}
% El alcance de esta investigación descriptiva se enfoca en proporcionar una visión detallada y exhaustiva de la aplicación de un sistema inteligente de gestión de compras basado en el histórico de ventas en un local gastronómico ubicado en Ciudad del Este. 
% Se describirá el proceso de recopilación de datos históricos de ventas, incluyendo la fuente de los datos, el periodo de tiempo cubierto y la naturaleza de la información registrada.
% Se proporcionará una descripción completa de cómo opera el sistema inteligente computarizado, incluyendo su arquitectura, algoritmos de pronóstico, y la forma en que utiliza los datos históricos para generar predicciones de demanda.


%VARIABLES DE ENTRADA Y SALIDA
\subsection{Definición de las variables de entrada y salida}

Se han considerado estas variables como variables de entrada debido a que, a través del análisis de la serie temporal de ventas, se han identificado patrones y tendencias significativas que impactan en las ventas del producto. El mes, el día del mes, el día de la semana, la presencia de días festivos y la estación del año influyen directamente en el comportamiento de las ventas. El promedio de venta mensual varía, lo que indica una estacionalidad en la demanda. Las ventas tienden a aumentar hacia el final e inicio del mes, posiblemente relacionado con el ciclo de pagos de los consumidores. La influencia de los días de la semana sugiere que los fines de semana son momentos clave para las ventas. La presencia de días festivos genera picos en las ventas, lo que puede estar vinculado a la celebración y al aumento de la demanda en esas fechas. Por último, la estación del año también desencadena cambios en las ventas, con un aumento en verano, lo que respalda la necesidad de considerar esta variable en el modelo para una predicción más precisa.
\vspace{1\baselineskip}
Teniendo todas estas informaciones, se procede a la conversión de estas características en variables de entrada y salida para el aprendizaje del modelo, y se asegura de que estén representadas en formato numérico.

\begin{table}[H]
    \begin{tabular}{|c|l|l|}  % Usamos "l" para alinear a la izquierda
      \hline
      \rowcolor{gray!50} \textbf{Columna} & \textbf{Descripción} & \textbf{Dato numerico} \\
      \hline
      mes &  variable mes Rango & 1-12\\
      dia & variable dias del mes Rango & 1-31\\
      dia\_semana & variable dia de la semana Rango & 0-6\\
      dia\_festivo & variable dia festivo Rango& 0-1\\
      estacion & variable estaciones del año Rango& 0-3\\
      \hline
    \end{tabular}
    \centering
    \caption{ Variables de entrada}
    \label{tab:variables_de _entrada} % Asigna una etiqueta a la tabla
  \end{table}


  
La siguiente consulta SQL realiza un análisis de ventas de un producto con id\_producto igual a 49. Agrupa los datos por fecha de venta y calcula diversas métricas relacionadas con la fecha, como el mes, el día, el día de la semana y si la fecha es un día festivo. También determina la estación correspondiente a cada fecha y agrega información sobre la cantidad total de ventas y la suma de las cantidades vendidas para cada día. En resumen, esta consulta proporciona un resumen detallado de las ventas del producto 49, organizado por fecha y con métricas relacionadas con la fecha para su posterior utilización.

\begin{verbatim}
  SELECT MONTH(fecha_venta) AS mes, 
  DAY(fecha_venta) AS dia, 
  DAYOFWEEK(fecha_venta) AS dia_semana, 
  IFNULL((SELECT 1 FROM dias_festivos df 
  WHERE CAST(v.fecha_venta AS date) = df.fecha),0) AS dia_festivo, 
  (SELECT estacion FROM estaciones e 
  WHERE cast(v.fecha_venta AS date) 
  BETWEEN e.fecha_inicio AND e.fecha_fin) AS estacion,
  COUNT(id_venta) AS cantidad_ventas, 
  SUM(v.cantidad) AS cantidad 
  FROM ventas v 
  WHERE id_producto = 49 
  GROUP BY CAST(fecha_venta AS date)
\end{verbatim}

Tabla (\ref{fig:tabla_resultante})resultante en formato .csv con el exportador de MySQL
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./tabla_resultante.png}
    \caption{Tabla resultante con las variables de entrada}
    \label{fig:tabla_resultante}
  \end{center}
\end{figure}

%LECTURA Y ESCALADO

\subsection{Lectura y escalado del dataset en Python}
En este trabajo se realiza todo el proceso en el software Python.

\vspace{1\baselineskip}
La elección de Python como lenguaje de programación se fundamenta en su destacado uso en el campo de la inteligencia artificial, respaldado por una amplia gama de bibliotecas y una comunidad activa\cite{mirjalili2020python}. En cuanto al entorno de pruebas, se ha optado por Google Colab debido a su facilidad de uso y su gratuidad, lo que lo convierte en una opción conveniente para el desarrollo y la prueba de modelos de inteligencia artificial.

\vspace{1\baselineskip}
Los datos se preparan de manera similar tanto para las pruebas de regresión lineal como para las de LSTM, lo que implica el proceso de escalado y la división en variables de entrada y salida. En Python, se emplea la biblioteca pandas para la lectura de archivos CSV, y para el escalado, se recurre al MinMaxScaler de la librería sklearn (Figura: \ref{fig:lectura_escalado}).

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.90]{./lectura_escalado.jpg}
    \caption{Lectura y escalado del dataset en Python}
    \label{fig:lectura_escalado}
  \end{center}
\end{figure}

\vspace{1\baselineskip}
En el grafico \ref{fig:grafico_violin} se puede observar que todas las variables de entrada estan escaladas entre 0 y 1 incluyendo el set train,validación y test.
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.50]{./grafico tipo violin de escalado.png}
    \caption{Grafico en violín incluyendo los tres set train, validación y test}
    \label{fig:grafico_violin}
  \end{center}
\end{figure}

En el grafico \ref{fig:grafico_violin_salida} se puede observar que los datos de salida también estén escaladas correctamente.
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.50]{./grafico_violin_salida.jpg}
    \caption{Gráfico en violín de los datos de salida escaladas}
    \label{fig:grafico_violin_salida}
  \end{center}
\end{figure}

%DISTRIBUCION DE LOS DATOS
\subsection{Distribución del conjunto de datos}

En el proceso de entrenamiento, es fundamental dividir el conjunto de datos en tres conjuntos distintos:

\begin{itemize}
  \item \textbf{Conjunto de Entrenamiento (Train)}: Este conjunto se utiliza para entrenar el modelo, lo que significa que el modelo aprenderá de estos datos durante el proceso de entrenamiento.

  \item \textbf{Conjunto de Validación (Val)}: El conjunto de validación se emplea para evaluar si el modelo está sobreajustando los datos de entrenamiento. Ayuda a ajustar parámetros y prevenir el sobreajuste.

  \item \textbf{Conjunto de Prueba (Test)}: Este conjunto no se utiliza durante el entrenamiento del modelo y contiene datos con valores desconocidos. Se utiliza al final para evaluar la precisión del modelo en un entorno real, ya que no ha tenido acceso a estos datos previamente.
\end{itemize}

Se toma inicialmente 70\% train, 15\% val y 15\% test, como valores comúnmente utilizados para estos casos.
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.75]{./divisionde _datos.jpg}
    \caption{Algoritmo de distribución de los datos.}
    \label{fig:distribucion_algoritmo}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.40]{./serie_normal_dividida.png}
    \caption{Distribución de los datos, ventas totales por día.}
    \label{fig:distribucion_datos}
  \end{center}
\end{figure}



%APLICACION Y COMPARACION
\section{Aplicación y comparación de técnicas ML}

%REGRESION LINEAL
\subsection{Regresión lineal}
Para la implementación de regresión lineal, se ha optado por utilizar el SGDRegressor de la biblioteca sklearn. En lo que respecta a la elección de hiperparámetros, se han seleccionado valores comunes para un modelo de regresión lineal implementado a través del algoritmo de Descenso de Gradiente Estocástico (SGD). Estos hiperparámetros son los siguientes:

\begin{itemize}
  \item \texttt{loss=“squaredro \_loss”}: Se ha elegido la función de pérdida de error cuadrático medio (MSE) debido a la naturaleza de un problema de regresión. La función MSE evalúa la discrepancia al cuadrado entre las predicciones y los valores reales, y se minimiza durante el proceso de entrenamiento.

  \item \texttt{penalty=“l2”}: Se ha especificado la regularización Ridge, que es una técnica comúnmente utilizada para abordar problemas de regresión. La regularización Ridge ayuda a prevenir el sobreajuste incorporando un término de penalización en la función de pérdida.

  \item \texttt{alpha=0.0001}: El hiperparámetro de regularización controla la intensidad de la regularización. En este caso, se ha optado por un valor pequeño (0.0001) para evitar una regularización excesiva y permitir que el modelo se ajuste de manera adecuada a los datos.

  \item \texttt{max\_iter=1000}: Se ha establecido el número máximo de iteraciones permitidas para el descenso de gradiente en 1000, un valor suficientemente amplio para asegurar la convergencia del modelo.

  \item \texttt{tol=1e-3}: La tolerancia se utiliza para determinar cuándo se ha alcanzado la convergencia. Si el cambio en la pérdida entre iteraciones sucesivas es menor que 1e-3, el algoritmo se detendrá. Este valor de tolerancia es apropiado para garantizar una convergencia razonable.

  \item \texttt{learning\_rate=“constant”}: Se ha configurado la tasa de aprendizaje como constante a lo largo del proceso de entrenamiento.
\end{itemize}

\begin{figure}[H]
    \begin{center}
      \includegraphics[scale=0.50]{./regresion lineal estructura.png}
      \caption{Algoritmo de la estructura de regresión lineal.}
      \label{fig:estructura_lineal}
    \end{center}
  \end{figure}

  A pesar de que tanto el Error Cuadrático Medio (MSE) como el Error Absoluto Medio (MAE) proporcionan indicadores favorables, el valor de R-cuadrado (R²) indica que el modelo se encuentra notablemente alejado de la excelencia representada por un valor de 1, ya que R² = 1 constituye la condición ideal (Figura \ref{fig:grafico_violin_salida} ) .

  \begin{figure}[H]
    \begin{center}
      \includegraphics[scale=0.70]{./metricas de error REGRESION LINEAL.png}
      \caption{Metrica de error regresión lineal.}
      \label{fig:metricas_regresion}
    \end{center}
  \end{figure}

  \begin{figure}[H]
    \begin{center}
      \includegraphics[scale=0.60]{./Grafico de predicciones con regresión.png}
      \caption{Grafico de predicción con regresión lineal.}
      \label{fig:prediccion_regresion}
    \end{center}
  \end{figure}

  \begin{figure}[H]
    \begin{center}
      \includegraphics[scale=0.50]{./modeloLineal7Dias.jpg}
      \caption{Predicción de 7 días con regresión lineal.}
      \label{fig:grafico_lstm}
    \end{center}
  \end{figure}


  %LSTM
\subsection{LSTM}

La red neuronal LSTM se modela con ayuda de la librería keras, es una librerıa de codigo abierto de redes neuronales, que se implementa sobre otros frameworks de aprendizaje automático como Tensorflow, CNTK o theano. Aparecio en 2015 y fue creada por Fran¸cois Chollet.
Tal y como se menciona en su documentación \cite{keras-doc}, Keras se guía por cuatro principios. Ser amigable para el usuario, modular, fácilmente extensible e implementada en python.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.40]{./arquitectura LSTM.png}
    \caption{Algoritmo de la estructura del LSTM.}
    \label{fig:estructura_lstm}
  \end{center}
\end{figure}

Al configurar una red neuronal LSTM, es común considerar varios hiperparámetros clave (Codigo \ref{fig:desenpeño}):

\begin{itemize}
  \item \textbf{Unidades de LSTM:} Por lo general, se comienza con un número moderado de unidades en las capas LSTM, como 64 o 128. Aumentar el número de unidades puede ayudar a capturar patrones más complejos, aunque conlleva un mayor riesgo de sobreajuste. En este caso, se obtuvieron los mejores resultados con 64 neuronas.

  \item \textbf{Número de capas LSTM:} Para problemas más complejos, es posible agregar capas adicionales. Las arquitecturas con dos capas LSTM son comunes y funcionan bien en muchas aplicaciones. Sin embargo, en este caso, se determinó que una sola capa oculta era la mejor opción.

  \item \textbf{Función de Activación:} Para las capas LSTM, la función de activación predeterminada suele ser “tanh”. No obstante, se pueden explorar otras funciones como “relu” o “sigmoid” según las necesidades del problema. En este caso, se lograron buenos resultados utilizando “relu” en la capa oculta y “linear” en la salida.

  \item \textbf{Función de Pérdida:} En problemas de regresión, es común utilizar el "mean squared error" (MSE) como función de pérdida. No obstante, la elección puede variar según el problema en particular. En este caso, se optó por el MSE debido a su versatilidad en problemas de regresión.

  \item \textbf{Optimizador:} El “Adam” es un optimizador sólido y ampliamente utilizado en problemas de aprendizaje profundo. Aunque existen otras opciones como “RMSprop,” la elección depende en gran medida del contexto. En esta configuración, se prefirió “RMSprop” por su relevancia en predicciones de demanda.

  \item \textbf{Tasa de Aprendizaje:} La tasa de aprendizaje es un hiperparámetro crucial. Por lo general, se comienza con un valor bajo, como 0.001, y se ajusta según la convergencia del modelo. En este caso, una tasa de aprendizaje de 0.01 resultó óptima.

  \item \textbf{Dropout:} La técnica de dropout, aplicada en un porcentaje determinado de neuronas durante el entrenamiento, puede ayudar a prevenir el sobreajuste. Se recomienda iniciar con un valor del 20\% y ajustar según las necesidades del problema. Para esta configuración, un dropout del 20\% fue suficiente.
\end{itemize}


% Es importante tener en cuenta que la elección de estos hiperparámetros puede depender en gran medida del problema específico y requiere experimentación para encontrar la configuración óptima.



\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.50]{./predicción 7 días.png}
    \caption{Grafico de predicción con LSTM.}
    \label{fig:grafico_lstm}
  \end{center}
\end{figure}

Los tres valores muestran una proximidad aceptable, lo que indica que el modelo tiene una capacidad de generalización satisfactoria (Figura \ref{fig:desenpeño}).
\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.50]{./comparativo desempeños LSTM.png}
    \caption{Comparativo de desempeños LSTM.}
    \label{fig:desenpeño}
  \end{center}
\end{figure}


En este trabajo para evaluar el rendimiento de una red neuronal LSTM se hace uso de la gráfica de pérdidas loss. Esta permite evaluar cómo es su evolución de los datos en un modelo. Según la literatura entre más grandes las pérdidas las predicciones van a estar alejadas del valor real. En la (Figura \ref{fig:perdida}) se puede observar la evolución de pérdidas a través de las épocas, se evidencia dos señales que corresponden a entrenamiento y validación marcados con los colores azul y naranja respectivamente, se infiere que los valores decrecen con una alta tasa hasta la época 5 aproximadamente y luego no presentan cambios significativos. Las pérdidas son de 0.020 y 0.039 para entrenamiento y test respectivamente.

\begin{figure}[H]
  \begin{center}
    \includegraphics[scale=0.50]{./grafico de perdida en las épocas.png}
    \caption{Grafico de perdida(loss) en las épocas LSTM .}
    \label{fig:perdida}
  \end{center}
\end{figure}

% \section{Afinación de hiperparámetros con base en porcentaje de pérdida RMS}




% \section{Analítica descriptiva}




% \section{Conjunto de datos}

% Cuenta con 3.840 registros de ventas que agrupados por fecha dan un total de 185 registros, que corresponden a la cantidad de ventas diarias del prodcuto mensionado organizados por fechas desde el 17 de abril de 2023 hasta el 15 de octubre de 2023. 

% Se  empleó  un  conjunto  de  datos  recopilados  a  partir  del  número  diario  de  productos vendidos,  obtenido  de la empresa gastronomica que proveyo los datos para el analisis y predicción de la misma, es importante destacar que el conjunto de datos es continuamente actualizado, sin embargo, para los propósitos de esta investigación, se consideró un conjunto de datos con 180 registros que cubren el período desde 17 de Abril del 2023 hasta 15 de Octubre del 2023 de un producto en especifico “hamburgesa simple”.

% Como el objetivo de este proyecto consiste en predecir las ventas del dia, se realiza una gráfica \ref{fig:serie_completa}, la cual nos permite ver la serie temporal de las ventas del producto mencionado. 



% Posteriormente, se dividió el conjunto de datos en tres grupos: entrenamiento, validación y prueba. El grupo de entrenamiento comprendió 148 registros que cubren desde 17 de abril hasta el 9 de septiembre. El grupo de validación incluyó 18 registros, desde 10 de septiembre hasta el 29 de septiembre  y el conjunto de prueba 19 registros desde el 30 de septiembre hasta 18 de octubre.



% \textbf{Lista de requisitos:}

% \begin{itemize}
% % \item Registro de usuarios(Alta, Baja, Modificaciones).
% \item Registro de productos (Alta, Baja, Modificaciones).
% \item Registro de insumos(materia prima) (Alta, Baja,Modificaciones). 
% \item Registro de pedidos/ventas (Alta, Baja, Modificaciones).
% \item Generación de Informes Personalizados: Debe permitir la creación de informes personalizados que muestren datos específicos para análisis.
% \item Análisis de Tendencias: Debe ser capaz de identificar tendencias y patrones en las ventas de insumos, lo que facilita la toma de decisiones basadas en datos.

% \end{itemize}
% \begin{figure}[H]
%     \begin{center}
%       \includegraphics[scale=0.90]{./diseño_procedural.png}
%       \caption{Diseño procedural.}
%       \label{fig:diseño_procedural}
%     \end{center}
%   \end{figure}
  
% \section{Librerias de uso común en Mching Learning}
% A continuación se instalara y actualizaran las principales librerias mas utilizadas cuando se realizan programas de inteligencia artificial.

% \begin{itemize}
%   \item \textbf{TensorFlow:} Es una plataforma de código abierto de un extremo a otro para el aprendizaje automático. Tiene un ecosistema integral y flexible de herramientas, bibliotecas y recursos comunitarios que permite a los investigadores impulsar lo último en ML y a los desarrolladores crear e implementar fácilmente aplicaciones basadas en ML.
%   Es ampliamente utilizada para desarrollar y entrenar modelos de redes neuronales, incluyendo modelos LSTM (Long Short-Term Memory).
%   \item \textbf{Keras:} Es una interfaz de alto nivel para TensorFlow que facilita la construcción, entrenamiento y evaluación de modelos de redes neuronales, incluyendo modelos LSTM.
% \end{itemize}



% La función de activación utilizada para modelar la no linealidad suele ser la Unidad Lineal Rectificada (ReLU), que puede calcularse más rápido que las funciones tangentes sigmoideas o hiperbólicas utilizadas tradicionalmente y también ofrece interesantes propiedades de convergencia.

% \subsection{Alcance exploratorio.}
% La medida de este alcance abarca la exploración de problemas generalmente poco conocidos, a veces difíciles de conocer.

% \subsection{Alcance descriptivo.}
% La medida de este alcance abarca la descripción del fenómeno, situación, contexto o evento; detalla cómo es y cómo se manifiesta. Busca especificar propiedades, características y rasgos importantes. Describe tendencias de un grupo o población. Es útil para mostrar con precisión los ángulos o dimensiones de un fenómeno, suceso, comunidad, contexto o situación.

% \subsection{Alcance correlacional.}
% La profundidad de este alcance busca establecer relaciones entre variables sin precisar sentido de causalidad, es decir, no analiza relación causal.

% Un ejemplo de este alcance es una investigación que busca averiguar cómo se relacionan las calificaciones de los alumnos de un grado, en las asignaturas: Castellano y Matemática.

% \subsection{Alcance explicativo.}
% La profundidad de este alcance busca establecer relaciones entre variables precisando sentido de causalidad, es decir, analiza relación entre causa y efecto entre variables.

% Un ejemplo de este alcance es una investigación que busca averiguar la relación entre urbanización y alfabetismo en un país, para ver qué variables macrosociales definen el grado de alfabetización de la población del país.

% \section{Diseño}
% Es el plan o estrategia que se desarrolla para obtener la información que se requiere en una investigación, generalmente para verificar la hipótesis. La precisión, amplitud y profundidad de la información obtenida varía en función del diseño elegido \cite{sampieri}.

% En la literatura sobre investigación cuantitativa es posible encontrar diferentes clasificaciones de los diseños; los autores \cite{sampieri} adoptan la siguiente clasificaciòn: investigación experimental e investigación no experimental. A su vez, la primera puede dividirse de acuerdo con las clásicas categorías de Campbell y Stanley (1966) en: preexperimentos, experimentos ``puros'' y cuasiexperimentos. La investigación no experimental, siempre de acuerdo con \cite{sampieri}, se subdivide en diseños transversales y diseños longitudinales.

\vspace{.5 cm}

% \textbf{Ejemplo de diseño en una investigación tecnológica formativa.}

% Aún más que en la investigación en ciencias básicas, es en la investigación tecnológica donde se puede apreciar la importancia del diseño para obtener un buen producto o servicio. Cabe entonces ilustrarlo con un ejemplo tomado dentro de esta última forma de investigación desde la referencia \cite{lan}.

\vspace{.5 cm}


% \textbf{\emph{Metodología para implementar red de área local. (\gls{lan}\@)}}\footnote{Por brevedad, solo se desarrolla la etapa de diseño.}




% Hoy en día, como nunca antes, el ser social necesita estar informado. Para estudiar problemas y tomas de decisiones es necesario disponer de datos precisos, en el lugar y en el instante preciso. En gran medida se logra lo anterior con las redes de computadoras, cuyo objetivo fundamental es compartir recursos e información pues ofrecen acceso a servicios universales de datos tales como: bases de datos, correo electrónico, transmisión de archivos y boletines electrónicos; eliminando el desplazamiento de los individuos en la búsqueda de información y aumentando la capacidad de almacenamiento disponible por cada usuario en un momento determinado.

% Un gran porcentaje de las redes de computadoras se usan para la transmisión de información científica siendo una vía rápida y económica de divulgar resultados y de discutir con otros especialistas afines sobre un tema en cuestión. En este trabajo en particular se aborda la metodología a seguir para la implementación de redes de computadoras de área local; las cuales cumplen todos los objetivos planteados a una escala reducida ya que son propiedad de una sola organización (un solo centro administrativo o fabril) abarcando zonas geográficas de algunos kilómetros como máximo. La experiencia en el campo de \glspl{lan} en el ámbito universitario, donde las mismas se emplean para la gestión administrativa y económica, para la transmisión de información científica y para la enseñanza; ha dejado claro que el diseño, la instalación y puesta a punto de una \gls{wan} suele ser un proceso cuidadoso del cual depende en grado sumo que se cumplan los objetivos para los que se invirtió en dicha red.

% Para su comprensión el trabajo se divide en cinco partes o etapas:
% \begin{itemize}
% \item Etapa de estudio,
% \item Etapa de diseño.\footnote{Solo se desarrolla esta etapa.}
% \item Etapa de elaboración de la solicitud de oferta y selección del vendedor,
% \item Etapa de instalación y puesta en funcionamiento,
% \item Etapa de análisis de las prestaciones y evaluación de los resultados.
% \end{itemize}
 
% Una vez concluida la primera etapa y aprobado el presupuesto de la red es necesario realizar la etapa del \textit{diseño} de la \gls{lan} para lo cual se deben seguir los siguientes pasos:

\renewcommand{\labelitemi}{$-$}

% \begin{itemize}
% \item Seleccionar la(s) topología(s) y norma(s) de red a emplear,
% \item Seleccionar el soporte de transmisión a utilizar,
% \item Analizar la necesidad de emplear técnicas de conectividad,
% \item Considerar ampliaciones futuras de la red,
% \item Realizar una evaluación primaria del tráfico,
% \item Contemplar las necesidades del personal involucrado en la red,
% \item Modificar, de ser necesario, el flujo de la información y seleccionar el software de aplicación.
% \end{itemize}

% \textit{Seleccionar la topología.} Este paso, el cual es dependiente de los resultados del anterior. Las tres topologías más empleadas son: bus, estrella y anillo; mientras que las normas más comunes son: Ethernet, Token Ring y ArcNet. La selección de los aspectos anteriores trae aparejado escoger la velocidad de transmisión, la distancia máxima a emplear, el método de control de acceso al medio, etc. La elección se realiza a partir de la necesidad particular y de un amplio conocimiento de las topologías y normas existentes. 

% \textit{Seleccionar el Soporte de Transmisión.} Esto está muy relacionado con la norma a emplear y con las características de los puntos a conectar. Es vital realizar una selección adecuada pues una opción equivocada comprometería la eficacia y la velocidad de la transferencia de datos. Para la elección de uno u otro medio de transmisión se debe tomar entre otras cosas las dimensiones de la instalación, el costo, la evolución tecnológica estimada, la facilidad de instalación y el grado de hostilidad electromagnética presente en el entorno. 

% Aunque el \gls{sored}  (del inglés NOS: Netware Operating System) NetWare predomina en el mundo, éste no es siempre la elección adecuada, debido a sus costos y características. En el mercado existen otros \glspl{sored} tales como: LAN Manager, LANServer, LANtastic, Vines, LINUX, Windows NT Server, Windows 2000 Server, etc.; los cuales poseen una determinada cuota de mercado. Para seleccionar el SOR adecuado se debe tener en cuenta:

% \begin{itemize}
% \item El nivel de confidencialidad que brinda a los datos,
% \item Si es del tipo cliente-servidor o de igual a igual,
% \item Grado de tolerancia a fallos que posee,
% \item Memoria RAM necesaria en el servidor y en las estaciones de trabajo,
% \item Facilidades de administración y diagnóstico que brinda,
% \item Si posee o no sistema de correo electrónico,
% \item Características de manipulación de colas de impresión.
% \end{itemize}

% \textit{Analizar la necesidad de emplear técnicas de conectividad.} Esto estará en función de las dimensiones de la organización, del tráfico a cursar y el tipo de equipamiento a interconectar entre otros aspectos. Es necesario conocer en profundidad dichas técnicas para realizar una adecuada selección entre repetidores, puentes, ruteadores, compuertas, servidores de acceso, etc. y lograr su correcta ubicación. La mejor solución muchas veces hace uso de más de un tipo de dispositivo de interconexión.

% \textit{Considerar ampliaciones futuras de la red.} Aún cuando de forma inmediata no sea necesario extender la red ni conectarse a otros, ésta debe poseer la base para que a partir de ella, y en cualquier momento sea posible una ampliación o llegar a formar parte de otras redes.

% \textit{Realizar una evaluación primaria del tráfico.} Aquí debe estimarse el tráfico que circulará en la red y analizar si el mismo no afecta el tiempo de acceso a la información ya otros recursos compartidos. Es importante que una vez instalada y puesta en funcionamiento la \gls{lan} se efectúen periódicamente estudios de este tipo.

% \textit{Contemplar las necesidades del personal involucrado en la red.} Esto es muy importante pues en última instancia éste será el personal que utilizará la red y por lo tanto deben quedar satisfechas sus necesidades de forma tal que la nueva red sea un elemento que facilite su trabajo.

% \textit{Modificar de ser necesario el flujo de información y seleccionar el software de aplicación.} Esto implica la modificación, como última opción, de la manera en que la información circula dentro de la organización y la definición del software de aplicación necesario, ya sea comercial o aquél que se encargará al personal especializado; que conozca las particularidades de la programación en ambiente multiusuario. El software encargado o adquirido debe ser de fácil instalación y aprendizaje. Además se debe velar porque sea posible tener acceso a posteriores actualizaciones y que éstas no sean caras.

% \section{Lenguajes de programación}

% \subsection{Python}
% Python es un lenguaje de programación potente y elegante
% que sea fácil de leer y de entender\cite{python2021python}.

% Tiene estructuras de datos de alto nivel eficientes y un simple pero efectivo sistema de programación orientado a objetos. La elegante sintaxis de Python y su tipado dinámico, junto a su naturaleza interpretada lo convierten en un lenguaje ideal para scripting y desarrollo rápido de aplicaciones en muchas áreas, para la mayoría de plataformas.

% Python se destaca en el ámbito de la inteligencia artificial debido a su capacidad para gestionar conjuntos de datos voluminosos y su facilidad de programación. Su sintaxis concisa y legible lo convierte en una elección ideal para crear prototipos de forma rápida, dinámica y comprensible.
% Además de su amplia popularidad, la razón principal para adentrarse en Python en el contexto del aprendizaje automático es, indiscutiblemente, su vasto ecosistema de recursos previamente desarrollados. 

% \paragraph{Pandas:}
% Es una biblioteca de código abierto que proporciona estructuras de datos flexibles y herramientas de análisis de datos para Python. Su principal estructura de datos es el DataFrame, que permite organizar datos en tablas bidimensionales. Pandas es ampliamente utilizado para la limpieza, manipulación y análisis de datos tabulares, lo que lo hace esencial en el análisis de datos.

% \paragraph{NumPy:}
% Es una biblioteca fundamental para el cálculo numérico en Python. Proporciona soporte para arreglos multidimensionales (llamados ndarrays) y una amplia gama de funciones matemáticas para operaciones en estos arreglos. NumPy es crucial para realizar cálculos numéricos eficientes y es la base de muchas otras bibliotecas de análisis de datos en Python.

% \paragraph{Matplotlib:}
% Es una biblioteca de visualización que permite crear gráficos estáticos de alta calidad. Proporciona una variedad de tipos de gráficos, desde gráficos de líneas y barras hasta histogramas y gráficos de dispersión. Matplotlib es ampliamente utilizado para visualizar datos y resultados en el análisis de datos.

% \paragraph{Seaborn:}
% Es una biblioteca de visualización basada en Matplotlib que simplifica la creación de gráficos estadísticos atractivos. Ofrece una interfaz de alto nivel para crear fácilmente visualizaciones complejas y atractivas, lo que la hace útil para el análisis exploratorio de datos y la presentación de resultados.

% \paragraph{Scikit-learn:}
% Es una biblioteca de aprendizaje automático que proporciona herramientas para tareas comunes de aprendizaje automático, como clasificación, regresión, clustering y selección de modelos. También incluye utilidades para la evaluación de modelos y el preprocesamiento de datos, lo que la convierte en una biblioteca esencial para el análisis de datos relacionados con el aprendizaje automático.

% \paragraph{StatsModels:}
% Es una biblioteca que se utiliza para realizar análisis estadísticos y modelado en Python. Está especialmente orientada a tareas como la regresión lineal y logística, análisis de series temporales y pruebas estadísticas. Es útil para realizar análisis estadísticos rigurosos en el contexto del análisis de datos.

% \paragraph{SciPy:}
% Es una extensión de NumPy que proporciona funcionalidades adicionales para cálculos científicos y técnicos. Incluye módulos para optimización, interpolación, integración numérica, álgebra lineal, transformadas y más. Es una biblioteca esencial para aplicaciones científicas y de ingeniería que involucran cálculos numéricos.

% \paragraph{Plotly:}
% Es una biblioteca que permite crear gráficos interactivos y visualizaciones web en Python. Es útil para crear visualizaciones interactivas para el análisis exploratorio de datos y la presentación de resultados en aplicaciones web.

% \paragraph{Bokeh:}
% Es otra biblioteca para crear gráficos interactivos y aplicaciones web basadas en datos. Ofrece una amplia gama de herramientas para crear visualizaciones interactivas y es especialmente útil para proyectos que requieren interfaces de usuario interactivas.

% \paragraph{Dask:}
% Es una biblioteca que se utiliza para el procesamiento paralelo y distribuido de datos en Python. Permite trabajar con conjuntos de datos grandes y realizar operaciones complejas de manera eficiente en clústeres de computadoras.

% \paragraph{TensorFlow y PyTorch:}
% Si bien estas bibliotecas son conocidas principalmente por el aprendizaje profundo, también se utilizan en tareas de procesamiento y análisis de datos cuando se trabajan con modelos de redes neuronales. Proporcionan herramientas para construir, entrenar y evaluar modelos de aprendizaje automático y profundo.

% Estas bibliotecas forman la base del ecosistema de análisis de datos en Python y son esenciales para una variedad de tareas en ciencia de datos, análisis de datos y aprendizaje automático.

% \subsection{PHP}
% Es un lenguaje de programación ampliamente utilizado en el desarrollo web. Se destaca por su capacidad para crear aplicaciones web dinámicas e interactivas\cite{cobo2005php}.

% Ofrece diversas ventajas que lo hacen atractivo en el desarrollo web:

% \paragraph{Facilidad de aprendizaje:}
% Tiene una sintaxis simple y legible, lo que facilita su aprendizaje, especialmente para principiantes en programación.
% \paragraph{Amplia comunidad y soporte:}
% Cuenta con una gran comunidad de desarrolladores y abundantes recursos en línea, incluyendo bibliotecas y frameworks, lo que agiliza el desarrollo.
% \paragraph{Interacción con bases de datos:}
% Brinda soporte integrado para la mayoría de las bases de datos populares, lo que lo hace ideal para aplicaciones que gestionan datos.
% \paragraph{Versatilidad:}
% Se integra fácilmente con HTML y otros lenguajes web, permitiendo una amplia gama de aplicaciones y funcionalidades.
% \paragraph{Plataforma cruzada:}
% Es compatible con varias plataformas y sistemas operativos, lo que lo hace altamente portátil.
